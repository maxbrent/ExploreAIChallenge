{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook done by:\n",
    "- Name: Yashveer Singh\n",
    "- email: yashveerm8@yahoo.com\n",
    "- contact: 072 217 9781\n",
    "- role: Senior Data Scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science End-to-End Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "Click on the link to be taken to the relevant section\n",
    "\n",
    "- [1. Exploratory Data Analysis](#1)\n",
    "    - [1.1. Measuring Data Skewness](#1.1)\n",
    "    - [1.2. Normalizing & Standardizing Data](#1.2)\n",
    "    - [1.3. Imputing Missing Values](#1.3)\n",
    "    - [1.4. Feature Generation](#1.4)\n",
    "- [2. Modelling from Explored Data](#2)\n",
    "    - [2.1. Build the Model](#2.1)\n",
    "    - [2.2. Ensemble Modelling](#2.2)\n",
    "    - [2.3. Hyperparameter Tuning](#2.3)\n",
    "    - [2.4. Feature Importance using SHAP](#2.4)\n",
    "    - [2.5. Pipelining](#2.5)\n",
    "- [3. Evaluating Models with Different Metrics](#3)\n",
    "- [4. Scaling up your Model to Production](#4)\n",
    "- [5. Advanced Methods in Statistics](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the Explore AI challenge I have decided to create a Notebook that covers various Data Science techniques used in practice. You will not find a notebook that covers all of these techniques, not even in **Kaggle Kernels!**\n",
    "- In this notebook I intend to give a complete overview of Data Science Methodologies that I have acquired through my many years of experience being a Data Scientist.\n",
    "- I will be using multiple datasets in this notebook to explain various concepts, since one dataset may not be enough to cover all the different concepts that I intend to cover.\n",
    "\n",
    "$$\\textbf{\n",
    "NB: There is a lot of content that I will cover in this notebook. Due to the short space of time, not all concepts may go into \n",
    "detail,}$$\n",
    "$$\\textbf{however I will provide addtional links for further explanation.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "- The purpose of Exploratory Data Analysis is to gain insight into our dataset\n",
    "- In Exploratory Data Analysis we:\n",
    "    1. Examine the Distribution of our Data.\n",
    "    2. Try to find new insights to our Data that will lead to better feature generation.\n",
    "    3. This new feature generation will lead us to build a model with a higher performance score.\n",
    "    4. Remember that Data Science is not about creating fancy plots of Data and fitting data to a model, but rather to gain insight from data that will lead to new features and increase overall model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "### 1.1 Measuring Data Skewness\n",
    "- This is the first method in the data processing step\n",
    "- Skewness of data refers to data that has a different mean, mode and median. When all of these values differ we say that our data is skew.\n",
    "- The reasons for moving skewness in data is so that when we feed our data into our model, this will prevent our model from being biased to specific features in our data. By removing this skewness we are ensuring that the presence of outliers in the data are tremendously reduced.\n",
    "- Further reading material on data skewness can be found [Top 3 Methods for Handling Skew Data](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45)\n",
    "- For this example, we will use a medical dataset with medical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "\n",
    "First load the dataset which will be used for training and testing the model built in the modelling section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maxbrent/ExploreAIChallenge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "!pip install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"/content/ExploreAIChallenge\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/content/ExploreAIChallenge/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "\n",
    "X = pd.read_csv('/content/ExploreAIChallenge/data/X_data.csv',index_col=0)\n",
    "y_df = pd.read_csv('/content/ExploreAIChallenge/data/y_data.csv',index_col=0)\n",
    "y = y_df['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` and `y` are Pandas DataFrames that hold the data for 6,000 diabetic patients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the Dataset\n",
    "\n",
    "The features (`X`) include the following fields:\n",
    "* Age: (years)\n",
    "* Systolic_BP: Systolic blood pressure (mmHg)\n",
    "* Diastolic_BP: Diastolic blood pressure (mmHg)\n",
    "* Cholesterol: (mg/DL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target (`y`) is an indicator of whether or not the patient developed retinopathy.\n",
    "\n",
    "* y = 1 : patient has retinopathy.\n",
    "* y = 0 : patient does not have retinopathy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the model, let's take a closer look at the distribution of the training data. To do this, split the data into train and test sets using a 75/25 split which is recommended practice.\n",
    "\n",
    "For this, let's use the built in function provided by sklearn library.  See the documentation for [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms of each column of `X_train` below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X.columns:\n",
    "    X_train_raw.loc[:, col].hist()\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the distributions above can see, the distributions have a generally bell shaped distribution, but with slight rightward skew.\n",
    "\n",
    "Many statistical models assume that the data is normally distributed, forming a symmetric Gaussian bell shape (with no skew) more like the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "data = np.random.normal(50,12, 5000)\n",
    "fitting_params = norm.fit(data)\n",
    "norm_dist_fitted = norm(*fitting_params)\n",
    "t = np.linspace(0,100, 100)\n",
    "plt.hist(data, bins=60, density=True)\n",
    "plt.plot(t, norm_dist_fitted.pdf(t))\n",
    "plt.title('Example of Normally Distributed Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform this data to be closer to a normal distribution by removing the skewness. One way to remove the skew is by applying the log function to the data.\n",
    "\n",
    "Let's plot the log of the feature variables to see that it produces the desired effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train_raw.columns:\n",
    "    np.log(X_train_raw.loc[:, col]).hist()\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above the data is more symmetric after taking the log and is closer to normally distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 Normalizing & Standardizing Data\n",
    "- The second method in the Data Processing step is Normalizing/Standardizing data\n",
    "<br>\n",
    "<br>\n",
    "- $\\textbf{Normalizing}$ data is the process of bringing our values winthin a range of 0 - 1 and no assumptions have been made about the data\n",
    "    - The reason for why you would want to normalize your features is if each of your feature values have various scaling length, e.g. feature 1: 0 - 100 in increments of 1, feature 2: 0 - 1 in increments of 0.01, feature 3: 0 - 100 000 in increments of 1000.\n",
    "    - As you can see each feature from features 1 to features 3 have different scaling values\n",
    "    - It is thus important to bring all of our features to one scale length before feeding it into the model preventing inaccurate model results.\n",
    "    -Normalizing is good for outliers since it brings all of our values closer together.\n",
    "<br>\n",
    "<br>\n",
    "- $\\textbf{Standardizing}$ assumes that your data has a Gaussain(bell curve) distribution. This does not strictly have to be true.\n",
    "    - $$\\overline{x} = \\frac{x - mean(x)}{std(x)}$$\n",
    "    - The formula above is used to standardize data:\n",
    "        - x represents our data value\n",
    "        - mean(x) represents the mean value for the each feature\n",
    "    - Standardizing data is sensitive to outliers, therefore outliers will have to be manually removed before applying standardization\n",
    "- Further information on Normalization and Standardization techniques can be found [How, When, and Why should you Normalize, Standardize, Rescale Your Data](https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff)\n",
    "\n",
    "What happens if we do not Standardize/Normalize our data ? If you look at the image below, when we do not normalize or standardize our data, we in turn optimizing for an objective fuction that looks like the image on the left shown below. We are therefore not optimizing our model to converge to a global minimum.\n",
    "\n",
    "It is thus important to standardize/normalize our data in order for our models objective fuction to converge to a global minimum shown in the image on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('/content/ExploreAIChallenge/images/optimization.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean-Normalize the Data\n",
    "\n",
    "Let's now transform our data so that the distributions are closer to standard normal distributions.\n",
    "\n",
    "First let's remove some of the skew from the distribution by using the log transformation.\n",
    "Then we will \"standardize\" the distribution so that it has a mean of zero and standard deviation of 1. Recall that a standard normal distribution has mean of zero and standard deviation of 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_standard_normal(df_train, df_test):\n",
    "    \"\"\"\n",
    "    In order to make the data closer to a normal distribution, take log\n",
    "    transforms to reduce the skew.\n",
    "    Then standardize the distribution with a mean of zero and standard deviation of 1. \n",
    "  \n",
    "    Args:\n",
    "      df_train (dataframe): unnormalized training data.\n",
    "      df_test (dataframe): unnormalized test data.\n",
    "  \n",
    "    Returns:\n",
    "      df_train_normalized (dateframe): normalized training data.\n",
    "      df_test_normalized (dataframe): normalized test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove skew by applying the log function to the train set, and to the test set\n",
    "    df_train_unskewed = np.log(df_train)\n",
    "    df_test_unskewed = np.log(df_test)\n",
    "    \n",
    "    #calculate the mean and standard deviation of the training set\n",
    "    mean = df_train_unskewed.mean(axis=0)\n",
    "    stdev = df_train_unskewed.std(axis=0)\n",
    "    \n",
    "    # standardize the training set\n",
    "    df_train_standardized = (df_train_unskewed - mean) / stdev\n",
    "    \n",
    "    # standardize the test set\n",
    "    df_test_standardized = (df_test_unskewed - mean) / stdev\n",
    "    \n",
    "    return df_train_standardized, df_test_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = make_standard_normal(X_train_raw, X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transforming the training and test sets, the training set to be centered at zero with a standard deviation of 1.\n",
    "\n",
    "Do not observe the test set during model training in order to avoid biasing the model training process.\n",
    "Let's now look at the distributions of the transformed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train.columns:\n",
    "    X_train[col].hist()\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "### 1.3 Imputing Missing Values\n",
    "- We will now use a more $\\textbf{complexed medical dataset}$ with $\\textbf{missing values}$\n",
    "\n",
    "- Looking at the data in `X_train` below, some of the data is missing: some values in the output of the previous cell are marked as `NaN` (\"not a number\").\n",
    "\n",
    "- Missing data is a common occurrence in data analysis, that can be due to a variety of reasons, such as measuring instrument malfunction, respondents not willing or not able to supply information, and errors in the data collection process.\n",
    "\n",
    "- Let's examine the missing data pattern.\n",
    "- First import necessary libraries for resolving missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import sklearn\n",
    "import itertools\n",
    "import pydotplus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image \n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "from util import load_data, cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, X_test, y_dev, y_test = load_data(10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.25, random_state=10)\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our $\\textbf{targets}$ `y` will be whether or not the target died within 10 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_train.isnull(), cbar=False)\n",
    "plt.title(\"Training\")\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(X_val.isnull(), cbar=False)\n",
    "plt.title(\"Validation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature, represented as a column, values that are present are shown in black, and missing values are set in a light color.\n",
    "\n",
    "From this plot, we can see that many values are missing for systolic blood pressure (`Systolic BP`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_rows_missing(df):\n",
    "    '''\n",
    "    Return percent of rows with any missing\n",
    "    data in the dataframe. \n",
    "    \n",
    "    Input:\n",
    "        df (dataframe): a pandas dataframe with potentially missing data\n",
    "    Output:\n",
    "        frac_missing (float): fraction of rows with missing data\n",
    "    '''\n",
    "    return sum(df.isnull().any(axis=1))/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComputed fraction missing: {}\".format(fraction_rows_missing(X_dev)))\n",
    "print(f\"Fraction of rows missing from X_train: {fraction_rows_missing(X_train):.3f}\")\n",
    "print(f\"Fraction of rows missing from X_val: {fraction_rows_missing(X_val):.3f}\")\n",
    "print(f\"Fraction of rows missing from X_test: {fraction_rows_missing(X_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see from the above results that the majority of the data is missing in each row\n",
    "- We will now examine the distributions of the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_rows = X_train[X_train.isnull().any(axis=1)]\n",
    "\n",
    "columns_except_Systolic_BP = [col for col in X_train.columns if col not in ['Systolic BP']]\n",
    "\n",
    "for col in columns_except_Systolic_BP:\n",
    "    sns.distplot(X_train.loc[:, col], norm_hist=True, kde=False, label='full data')\n",
    "    sns.distplot(dropped_rows.loc[:, col], norm_hist=True, kde=False, label='without missing data')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the features are distributed similarly whether or not rows have been discarded with missing data. In other words missingness of the data is independent of these features.\n",
    "<br>\n",
    "<br>\n",
    "- But when considering the age feature, much more data tends to be missing for patients over 65. The reason could be that blood pressure was measured less frequently for old people to avoid placing additional burden on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform a 2 step process where we build our AI model and evaluate it:\n",
    "    - to dropped rows of missing data\n",
    "    - using an imputer fill in the missing data and evaluate the difference in performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2. Modelling from Explored Data\n",
    "- The purpose of Modelling is to build predictive/Artificial Intelligence models using the current data that we have to train our model.\n",
    "- The next phase is to use our trained model to make predictions on unseen data that we have not seen before.\n",
    "- These predictions can then be used to provide advice to health specialists on a patients medical condition.\n",
    "- In this section we will build a Decision Tree Model\n",
    "- Next we will use an Ensemble of Decision Tree Models known as Ensemble Modelling\n",
    "- We will evaluate our model on our missing data rows that have been dropped and compare the results of using an imputer for misssing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First begin with a **complete case analysis**, dropping all of the rows with any missing data. Run the following cell to drop these rows from our train and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dropped = X_train.dropna(axis='rows')\n",
    "y_train_dropped = y_train.loc[X_train_dropped.index]\n",
    "X_val_dropped = X_val.dropna(axis='rows')\n",
    "y_val_dropped = y_val.loc[X_val_dropped.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 Build the Model\n",
    "The first model that we will use is a simple Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=None, random_state=10)\n",
    "dt.fit(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The model will be evaluated using a c-index metric which I have coded in the util file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = dt.predict_proba(X_train_dropped)[:, 1]\n",
    "print(f\"Train C-Index: {cindex(y_train_dropped.values, y_train_preds)}\")\n",
    "\n",
    "\n",
    "y_val_preds = dt.predict_proba(X_val_dropped)[:, 1]\n",
    "print(f\"Val C-Index: {cindex(y_val_dropped.values, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the tree seems to be overfitting: it fits the training data so closely that it doesn't generalize well to other samples such as those from the validation set.\n",
    "\n",
    "To handle this, some of the hyperparameters will be changed in the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_hyperparams = {\n",
    "    \n",
    "    'max_depth':2,\n",
    "    'min_samples_split': 2\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reg = DecisionTreeClassifier(**dt_hyperparams, random_state=10)\n",
    "dt_reg.fit(X_train_dropped, y_train_dropped)\n",
    "\n",
    "y_train_preds = dt_reg.predict_proba(X_train_dropped)[:, 1]\n",
    "y_val_preds = dt_reg.predict_proba(X_val_dropped)[:, 1]\n",
    "print(f\"Train C-Index: {cindex(y_train_dropped.values, y_train_preds)}\")\n",
    "print(f\"Val C-Index (expected > 0.6): {cindex(y_val_dropped.values, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that by adjusting our hyperparameters, we prevented our model from overfitting our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 Ensemble Modelling\n",
    "- One Model may not be good enough to fit our data \n",
    "- Ensemble Modelling involves creating multiple models and using these multiple models to fit our data.\n",
    "- What are ensemble models good for ?\n",
    "- Ensemble models are good for reducing variance(overfitting) in your model accuracy\n",
    "- Additional information can be found here [Simple Guide for Ensemble Learning Methods](https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2)\n",
    "- In the above model we have built a Decision Tree model. We will now build multiple Decision Tree models or an Ensemble of Decision Trees called a Random Forest.\n",
    "- No matter how you choose hyperparameters, a single decision tree is prone to overfitting. So using random forests would be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=10)\n",
    "rf.fit(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just like how we computed C-Index metric for the decision tree above, we will now evaluate the C-Index metric using our ensemble model which is a random forest on the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_preds = rf.predict_proba(X_train_dropped)[:, 1]\n",
    "print(f\"Train C-Index: {cindex(y_train_dropped.values, y_train_rf_preds)}\")\n",
    "\n",
    "y_val_rf_preds = rf.predict_proba(X_val_dropped)[:, 1]\n",
    "print(f\"Val C-Index: {cindex(y_val_dropped.values, y_val_rf_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 Hyperparameter Tuning\n",
    "\n",
    "Training a random forest with the default hyperparameters results in a model that has better predictive performance than individual decision trees as in results displayed for decision trees, but this model is still overfitting.\n",
    "\n",
    "Therefore it is necessary to tune (or optimize) the hyperparameters, to find a model that both has good predictive performance and minimizes overfitting.\n",
    "\n",
    "The hyperparameters chosen to adjust for simplicity will be:\n",
    "\n",
    "- `n_estimators`: the number of trees used in the forest.\n",
    "- `max_depth`: the maximum depth of each tree.\n",
    "- `min_samples_leaf`: the minimum number (if `int`) or proportion (if `float`) of samples in a leaf.\n",
    "\n",
    "The approach implemented to tune the hyperparameters is known as a grid search:\n",
    "\n",
    "- Define a set of possible values for each of the target hyperparameters.\n",
    "\n",
    "- A model is trained and evaluated for every possible combination of hyperparameters.\n",
    "\n",
    "- The best performing set of hyperparameters is returned.\n",
    "\n",
    "Below is an implementation for a hyperparameter grid search, using the C-Index to evaluate each tested model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_grid_search(clf, X_train_hp, y_train_hp, X_val_hp, y_val_hp, hyperparams, fixed_hyperparams={}):\n",
    "    '''\n",
    "    Hyperparameter grid search on validation set.\n",
    "    Hyperparameters are input as a dictionary mapping each hyperparameter name to the\n",
    "    range of values to iterate over..\n",
    "\n",
    "    Input:\n",
    "        clf: sklearn classifier\n",
    "        X_train_hp (dataframe): dataframe for training set input variables\n",
    "        y_train_hp (dataframe): dataframe for training set targets\n",
    "        X_val_hp (dataframe): dataframe for validation set input variables\n",
    "        y_val_hp (dataframe): dataframe for validation set targets\n",
    "        hyperparams (dict): hyperparameter dictionary mapping hyperparameter\n",
    "                            names to range of values for grid search\n",
    "        fixed_hyperparams (dict): dictionary of fixed hyperparameters that\n",
    "                                  are not included in the grid search\n",
    "\n",
    "    Output:\n",
    "        best_estimator (sklearn classifier): fitted sklearn classifier with best performance on\n",
    "                                             validation set\n",
    "        best_hyperparams (dict): hyperparameter dictionary mapping hyperparameter\n",
    "                                 names to values in best_estimator\n",
    "    '''\n",
    "    best_estimator = None\n",
    "    best_hyperparams = {}\n",
    "    \n",
    "    # hold best running score\n",
    "    best_score = 0.0\n",
    "\n",
    "    # list of param values\n",
    "    lists = hyperparams.values()\n",
    "    \n",
    "    # get all param combinations\n",
    "    param_combinations = list(itertools.product(*lists))\n",
    "    total_param_combinations = len(param_combinations)\n",
    "\n",
    "    # iterate through param combinations\n",
    "    for i, params in enumerate(param_combinations, 1):\n",
    "        \n",
    "        param_dict = {}\n",
    "        for param_index, param_name in enumerate(hyperparams):\n",
    "            param_dict[param_name] = params[param_index]\n",
    "            \n",
    "        # create estimator with specified params\n",
    "        estimator = clf(**param_dict, **fixed_hyperparams)\n",
    "\n",
    "        # fit estimator\n",
    "        estimator.fit(X_train_hp, y_train_hp)\n",
    "        \n",
    "        # get predictions on validation set\n",
    "        preds = estimator.predict_proba(X_val_hp)\n",
    "        \n",
    "        # compute cindex for predictions\n",
    "        estimator_score = cindex(y_val_hp, preds[:,1])\n",
    "\n",
    "        print(f'[{i}/{total_param_combinations}] {param_dict}')\n",
    "        print(f'Val C-Index: {estimator_score}\\n')\n",
    "\n",
    "        # if new high score, update high score, best estimator\n",
    "        # and best params \n",
    "        if estimator_score >= best_score:\n",
    "                best_score = estimator_score\n",
    "                best_estimator = estimator\n",
    "                best_hyperparams = param_dict\n",
    "\n",
    "    # add fixed hyperparamters to best combination of variable hyperparameters\n",
    "    best_hyperparams.update(fixed_hyperparams)\n",
    "    \n",
    "    return best_estimator, best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a range for Hyperparameters Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_grid_search(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped):\n",
    "\n",
    "    # Define ranges for the chosen random forest hyperparameters \n",
    "    hyperparams = {\n",
    "\n",
    "        # how many trees should be in the forest (int)\n",
    "        'n_estimators': [100, 200, 300],\n",
    "\n",
    "        # the maximum depth of trees in the forest (int)\n",
    "        'max_depth': [i for i in range(1, 10)],\n",
    "        \n",
    "        # the minimum number of samples in a leaf\n",
    "        'min_samples_leaf': [1],\n",
    "\n",
    "    }\n",
    "\n",
    "    \n",
    "    fixed_hyperparams = {\n",
    "        'random_state': 10,\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier\n",
    "\n",
    "    best_rf, best_hyperparams = holdout_grid_search(rf, X_train_dropped, y_train_dropped,\n",
    "                                                    X_val_dropped, y_val_dropped, hyperparams,\n",
    "                                                    fixed_hyperparams)\n",
    "\n",
    "    print(f\"Best hyperparameters:\\n{best_hyperparams}\")\n",
    "\n",
    "    \n",
    "    y_train_best = best_rf.predict_proba(X_train_dropped)[:, 1]\n",
    "    print(f\"Train C-Index: {cindex(y_train_dropped, y_train_best)}\")\n",
    "\n",
    "    y_val_best = best_rf.predict_proba(X_val_dropped)[:, 1]\n",
    "    print(f\"Val C-Index: {cindex(y_val_dropped, y_val_best)}\")\n",
    "    \n",
    "    # add fixed hyperparamters to best combination of variable hyperparameters\n",
    "    best_hyperparams.update(fixed_hyperparams)\n",
    "    \n",
    "    return best_rf, best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf, best_hyperparams = random_forest_grid_search(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now the model will be evaluated on the test set. This is a crucial step, as trying out many combinations of hyperparameters and evaluating them on the validation set could result in a model that ends up overfitting the validation set. \n",
    "- So there is a need to check if the model performs well on unseen data, which is the role of the test set, which has been held out until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_best = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Test C-Index: {cindex(y_test.values, y_test_best)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Test set is a decent model value which is better than all of the previous models build before\n",
    "- In section [1.3](#1.3) I mentioned that we will evaluate our model on dropped rows of missing data and as well as use an Imputer to fill in this missing data and evaluate the performance.\n",
    "- Let's now conduct this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputer for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute using regression on other features\n",
    "imputer = IterativeImputer(random_state=0, sample_posterior=False, max_iter=1, min_value=0)\n",
    "imputer.fit(X_train)\n",
    "X_train_imputed = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)\n",
    "X_val_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data will now be retrained using the same hyperparameter tuning that was done in the model building section, the only difference is that now the missing values have been replace using an Imputer instead of dropping the rows. Model performance will be evaluated on the missing data that has now been filled to see a comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "\n",
    "    # how many trees should be in the forest (int)\n",
    "    'n_estimators': [i for i in range(100, 1000, 100)],\n",
    "\n",
    "    # the maximum depth of trees in the forest (int)\n",
    "    'max_depth': [j for j in range(1, 10)],\n",
    "\n",
    "    # the minimum number of samples in a leaf\n",
    "    'min_samples_leaf': [1],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier\n",
    "\n",
    "rf_imputed, best_hyperparams_imputed = holdout_grid_search(rf, X_train_imputed, y_train,\n",
    "                                                           X_val_imputed, y_val,\n",
    "                                                           hyperparams, {'random_state': 10})\n",
    "\n",
    "print(\"Performance for best hyperparameters:\")\n",
    "\n",
    "y_train_best = rf_imputed.predict_proba(X_train_imputed)[:, 1]\n",
    "print(f\"- Train C-Index: {cindex(y_train, y_train_best):.4f}\")\n",
    "\n",
    "y_val_best = rf_imputed.predict_proba(X_val_imputed)[:, 1]\n",
    "print(f\"- Val C-Index: {cindex(y_val, y_val_best):.4f}\")\n",
    "\n",
    "y_test_imp = rf_imputed.predict_proba(X_test)[:, 1]\n",
    "print(f\"- Test C-Index: {cindex(y_test, y_test_imp):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_subset(forest, X_test, y_test):\n",
    "    # define mask to select large subset with poor performance\n",
    "    # currently mask defines the entire set\n",
    "    \n",
    "    mask = X_test['Age'] > 65\n",
    "\n",
    "    X_subgroup = X_test[mask]\n",
    "    y_subgroup = y_test[mask]\n",
    "    subgroup_size = len(X_subgroup)\n",
    "\n",
    "    y_subgroup_preds = forest.predict_proba(X_subgroup)[:, 1]\n",
    "    performance = cindex(y_subgroup.values, y_subgroup_preds)\n",
    "    \n",
    "    return performance, subgroup_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance, subgroup_size = bad_subset(best_rf, X_test, y_test)\n",
    "print(f\"C-Index (no imputation): {performance}\")\n",
    "\n",
    "performance, subgroup_size = bad_subset(rf_imputed, X_test, y_test)\n",
    "print(f\"C-Index (multivariate feature imputation): {performance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- According to the results above we can now see the result of [section 1.3](#1.3) where the rows with missing data was dropped\n",
    "- The result where we filled in the missing values using an Imputer has a better performance on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### 2.4 Feature Importance using SHAP\n",
    "- Why would we care about feature importance ? Surely our predictions from our model would be enough to determine what decisions should me made next.\n",
    "- This is false. The perception of just using a models predictions to make decisions could cost people their lives.\n",
    "- If we take the medical field for example, using a models prediction for a patients risk score to determine what treatment a patient will receive could kill the patient if it is just based on their risk score.\n",
    "- If we take another example such as the insurance industry, based on a persons risk profile if we issue products just based on the models predictions of a persons risk profile, we could end up issuing the wrong customer products, which could later down the line impact the company as having a bad reputation.\n",
    "<br>\n",
    "<br>\n",
    "- The purpose of feature importance is to understand which features have an effect on the models overall performance.\n",
    "- In this notebook a model was created to evaluate a patients risk score of dying within 10 years, but by understanding which features contribute to driving up the risk score such as age, sex, etc. a doctor will then be able to determine that maybe the age factor is driving up a patients risk score and will thus be able to prescribe the recommended treatment.\n",
    "<br>\n",
    "<br>\n",
    "- In this section I will try to explain the predictions using a slightly more sophisticated technique using $\\textbf{SHAP (SHapley Additive exPlanations), a cutting edge method that explains predictions made by black-box machine learning models }$\n",
    "<br>\n",
    "<br>\n",
    "Given a prediction made by a machine learning model, SHAP values explain the prediction by quantifying the additive importance of each feature to the prediction. SHAP values have their roots in cooperative game theory, where Shapley values are used to quantify the contribution of each player to the game.\n",
    "<br>\n",
    "<br>\n",
    "Although it is computationally expensive to compute SHAP values for general black-box models, in the case of trees and forests there exists a fast polynomial-time algorithm. For more details, see the: \n",
    "- [TreeShap paper](https://arxiv.org/pdf/1802.03888.pdf) \n",
    "\n",
    "Let's now use the [shap library](https://github.com/slundberg/shap) to do this for our random forest model. The code below will output the most at risk individuals in the test set according to our model using a force plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_risk = X_test.copy(deep=True)\n",
    "X_test_risk.loc[:, 'risk'] = rf_imputed.predict_proba(X_test_risk)[:, 1]\n",
    "X_test_risk = X_test_risk.sort_values(by='risk', ascending=False)\n",
    "X_test_risk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf_imputed)\n",
    "i = 0\n",
    "shap_value = explainer.shap_values(X_test.loc[X_test_risk.index[i], :])[1]\n",
    "shap.force_plot(explainer.expected_value[1], shap_value, feature_names=X_test.columns, matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the force Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The red sections on the left are features which push the model towards the final prediction in the positive direction (i.e. a higher Age increases the predicted risk).\n",
    "- The blue sections on the right are features that push the model towards the final prediction in the negative direction (if an increase in a feature leads to a lower risk, it will be shown in blue).\n",
    "- Note that the exact output of the chart will differ depending on the hyper-parameters that I chose for this model.\n",
    "- It is also possible to aggregate the models output and then use SHAP values to understand this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap.TreeExplainer(rf_imputed).shap_values(X_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colors indicate the value of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly it is seen that being a woman (`sex = 2.0`, as opposed to men for which `sex = 1.0`) has a negative SHAP value, meaning that it reduces the risk of dying within 10 years. High age and high systolic blood pressure have positive SHAP values, and are therefore related to increased mortality. \n",
    "\n",
    "You can see how features interact using dependence plots. These plot the SHAP value for a given feature for each data point, and color the points in using the value for another feature. This lets us begin to explain the variation in SHAP value for a single value of the main feature.\n",
    "\n",
    "Let's now see the interaction between Age and Sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Age', shap_values, X_test, interaction_index='Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that while Age > 50 is generally bad (positive SHAP value), being a woman generally reduces the impact of age. This makes sense since we know that women generally live longer than men.\n",
    "\n",
    "Let's now look at poverty index and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Poverty index', shap_values, X_test, interaction_index='Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that the impact of poverty index drops off quickly, and for higher income individuals age begins to explain much of variation in the impact of poverty index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
